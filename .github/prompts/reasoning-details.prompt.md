---
agent: agent
---
# GOAL
Your goal is implement this document to fix reasoning_details handling on #file:../../src/provider.ts
use this document as reference and documentation.

# Handling Multiple Reasoning_details Chunks

When working with streaming responses, reasoning_details are often delivered in multiple chunks. Here's how to handle them properly:

  1. Collect all reasoning_details chunks during streaming
  2. Preserve the complete sequence when sending them back to the API

## Collecting Reasoning_details During Streaming

Based on the search results, I'll explain how to handle multiple reasoning_details chunks when streaming responses from models that support reasoning tokens.

When working with streaming responses that include reasoning_details, you need to properly collect and preserve all chunks before sending them back to the API. This is especially important for maintaining reasoning continuity during tool calling.

# Collecting Reasoning_details During Streaming

When streaming responses with reasoning_details, you need to:

  1. Collect all reasoning_details chunks as they arrive
  2. Concatenate them in the correct order
  3. Preserve the complete sequence when sending them back to the API

Here's example how to implement this within openai:

```
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: 'YOUR_API_KEY',
});

async function streamWithReasoningDetails() {
  // Initialize variables to collect reasoning_details
  const allReasoningDetails = [];
  let assistantMessageContent = '';
  const toolCalls = [];

  // Stream the response
  const stream = await client.chat.completions.create({
    model: 'anthropic/claude-3.7-sonnet',
    messages: [
      {
        role: 'user',
        content: "What's the weather like in Boston? Then recommend what to wear.",
      },
    ],
    tools: [
      {
        type: 'function',
        function: {
          name: 'get_weather',
          description: 'Get current weather',
          parameters: {
            type: 'object',
            properties: {
              location: { type: 'string' },
            },
            required: ['location'],
          },
        },
      },
    ],
    reasoning: { max_tokens: 2000 },
    stream: true,
  });

  // Process the stream and collect all reasoning_details chunks
  for await (const chunk of stream) {
    // Collect reasoning_details if present in this chunk
    if (chunk.choices[0].delta.reasoning_details) {
      allReasoningDetails.push(chunk.choices[0].delta.reasoning_details);
    }

    // Collect content if present
    if (chunk.choices[0].delta.content) {
      assistantMessageContent += chunk.choices[0].delta.content;
    }

    // Collect tool_calls if present
    if (chunk.choices[0].delta.tool_calls) {
      for (const toolCall of chunk.choices[0].delta.tool_calls) {
        // Handle tool call collection (simplified)
        toolCalls.push(toolCall);
      }
    }
  }

  // Now use the collected reasoning_details in your next request
  const messages = [
    {
      role: 'user',
      content: "What's the weather like in Boston? Then recommend what to wear.",
    },
    {
      role: 'assistant',
      content: assistantMessageContent,
      tool_calls: toolCalls,
      reasoning_details: allReasoningDetails, // Pass the complete list of reasoning_details
    },
    {
      role: 'tool',
      tool_call_id: toolCalls[0].id,
      content: '{"temperature": 45, "condition": "rainy", "humidity": 85}',
    },
  ];

  // Second API call with preserved reasoning_details
  const response2 = await client.chat.completions.create({
    model: 'anthropic/claude-3.7-sonnet',
    messages,
    tools: [
      {
        type: 'function',
        function: {
          name: 'get_weather',
          description: 'Get current weather',
          parameters: {
            type: 'object',
            properties: {
              location: { type: 'string' },
            },
            required: ['location'],
          },
        },
      },
    ],
  });

  return response2;
}

```

# Important Considerations

1. **Preserve the Complete Sequence**: When multiple chunks contain reasoning_details, you must collect all of them and preserve the complete sequence when sending them back to the API.
2. **Don't Modify the Sequence**: The entire sequence of consecutive reasoning blocks must match the outputs generated by the model during the original request. You cannot rearrange or modify these blocks .
3. **Tool Calling Integration**: Preserving reasoning blocks is particularly important for tool calling. When models invoke tools, they pause their response construction to await external information. Including the original reasoning ensures the model can continue its reasoning from where it left off .
4. **Context Maintenance**: While tool results appear as user messages in the API structure, they're part of a continuous reasoning flow. Preserving reasoning blocks maintains this conceptual flow across multiple API calls .

# Example Use Case: Interleaved Thinking
Preserving reasoning_details is especially valuable for interleaved thinking, where models reason between tool calls. This enables more sophisticated decision-making after receiving tool results

With interleaved thinking, the model can:

1. Reason about the results of a tool call before deciding what to do next
2. Chain multiple tool calls with reasoning steps in between
3. Make more nuanced decisions based on intermediate results
4. Provide transparent reasoning for its tool selection process

By properly collecting and preserving reasoning_details across multiple chunks, you enable the model to maintain its reasoning continuity throughout complex multi-step interactions.

